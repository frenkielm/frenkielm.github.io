<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Mu Lin</title>

  <meta name="author" content="Mu Lin">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Mu Lin | 林沐
                  </p>

                  <p>Hi, I’m Mu Lin, an undergraduate student majoring in Computer Science and Technology at <a
                      href="https://www.sysu.edu.cn/sysuen/">Sun Yat-sen
                      University</a>, graduating in 2026. I will continue my academic journey as a Master's student at
                    <a href="https://www.sigs.tsinghua.edu.cn/en/">Shenzhen International Graduate School</a>, <a
                      href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>,
                    working with <a href="https://andytang15.github.io/">Prof. Yansong Tang</a>.
                  </p>
                  <p>My research focuses on <strong>robotic learning</strong> and <strong>computer vision</strong>,
                    particularly in <strong>dexterous grasping and manipulation</strong>. I am currently advised by <a
                      href="https://www.isee-ai.cn/~zhwshi/">Prof. Wei-Shi Zheng</a> and work with <a
                      href="https://wyl2077.github.io/">Yi-Lin Wei</a> at the Intelligence Science and System Lab
                    (ISEE).</p>
                  <p style="text-align:center">
                    <a href="mailto:linm67@mail2.sysu.edu.cn">Email</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=pb_HS6wAAAAJ&hl=zh-CN">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/frenkielm">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:25%;max-width:25%">
                  <a href="images/frenkie2.jpg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 10%;" alt="profile photo"
                      src="images/frenkie2.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Publications</h2>
                  <p>
                    *: equal contribution; †: corresponding author(s)
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="afforddexgrasp_stop()" onmouseover="afforddexgrasp_start()">
                <td style="padding:8px;width:25%;vertical-align:middle;">
                  <div class="one" style="position:relative; width:100%; height:150px;"> <!-- 添加相对定位容器 -->
                    <div class="two" id='afforddexgrasp_image'
                      style=" top:0; left:0; width:100%; height:100%; opacity:0; transition: opacity 0.3s ease;">
                      <img src='images/afforddexgrasp1.png' style="width:100%; height:100%; object-fit:contain;">
                    </div>
                    <img src='images/afforddexgrasp2.png'
                      style="width:100%; height:100%; object-fit:contain; top:0; left:0;">
                  </div>
                  <script type="text/javascript">
                    function afforddexgrasp_start() {
                      document.getElementById('afforddexgrasp_image').style.opacity = "1";
                    }
                    function afforddexgrasp_stop() {
                      document.getElementById('afforddexgrasp_image').style.opacity = "0";
                    }
                    afforddexgrasp_stop()
                  </script>
                </td>
                <td style="padding:8px;width:75%;vertical-align:middle;">
                  <a href="https://isee-laboratory.github.io/AffordDexGrasp/index.html">
                    <span class="papertitle">AffordDexGrasp: Open-set Language-guided Dexterous Grasp with
                      Generalizable-Instructive Affordance</span>
                  </a>
                  <br>
                  <a href="https://wyl2077.github.io/">Yi-Lin Wei*</a>,
                  <strong>Mu Lin*</strong>,
                  Yuhao Lin,
                  <a href="https://jianjian-jiang.github.io/">Jian-Jian Jiang</a>,
                  <a href="https://dravenalg.github.io/">Xiao-Ming Wu</a>,
                  <a href="https://www.lingan.art/"> Ling-An Zeng</a>,
                  <a href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng†</a>,
                  <br>
                  <em>ICCV</em>, 2025
                  <br>
                  <a href="https://isee-laboratory.github.io/AffordDexGrasp/index.html">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2503.07360">arXiv</a>
                  /
                  <a href="https://isee-laboratory.github.io/AffordDexGrasp/index.html">code(coming soon)</a>
                  <br>
                  <br>
                  Open-Set Language-guided dexterous grasp based on generalizable-instructive Affordance.
                </td>
              </tr>
              <tr onmouseout="omnidex_stop()" onmouseover="omnidex_start()">
                <td style="padding:8px;width:25%;vertical-align:middle;">
                  <div class="one" style="position:relative; width:100%; height:150px;">
                    <div class="two" id='omnidex_image'
                      style="top:0; left:0; width:100%; height:100%; opacity:0; transition: opacity 0.3s ease;">
                      <video muted autoplay loop style="width:100%; height:100%; object-fit:contain;">
                        <source src="images/omnidexgrasp.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/omnidexgrasp.png'
                      style="width:100%; height:100%; object-fit:contain; top:0; left:0;">
                  </div>
                  <script type="text/javascript">
                    function omnidex_start() {
                      document.getElementById('omnidex_image').style.opacity = "1";
                    }
                    function omnidex_stop() {
                      document.getElementById('omnidex_image').style.opacity = "0";
                    }
                    omnidex_stop()
                  </script>
                </td>
                <td style="padding:8px;width:75%;vertical-align:middle;">
                  <span class="papertitle">OmniDexGrasp: Generalizable Dexterous Grasping via Foundation Model and Force
                    Feedback</span>
                  <br>
                  <a href="https://wyl2077.github.io/">Yi-Lin Wei*</a>,
                  Zhexi Luo*,
                  Yuhao Lin,
                  <strong>Mu Lin*</strong>,
                  Zhizhao Liang,
                  Shuoyu Chen,
                  <a href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng†</a>
                  <br>
                  <em>under review</em>, 2025
                  <br>
                  arXiv page
                  /
                  project page
                  /
                  code
                  <p></p>
                  <p>
                    A generalizable dexterous framework that achieves omni-capabilities in user prompting, dexterous
                    embodiment, and grasping tasks.
                  </p>
                </td>
              </tr>

              <tr onmouseout="typetele_stop()" onmouseover="typetele_start()">
                <td style="padding:8px;width:25%;vertical-align:middle;">
                  <div class="one" style="align-items: center; height: 100%;">
                    <div class="two" id='ever_image' style=" align-items: center; height: 100%;">
                      <img src='images/typetele.png' width=115%>
                    </div>
                    <img src='images/typetele.png' width=115%>
                  </div>
                  <script type="text/javascript">
                    function typetele_start() {
                      document.getElementById('ever_image').style.opacity = "1";
                    }

                    function typetele_stop() {
                      document.getElementById('ever_image').style.opacity = "0";
                    }
                    typetele_stop()
                  </script>
                </td>
                <td style="padding:8px;width:75%;vertical-align:middle;">
                  <a href="https://isee-laboratory.github.io/TypeTele/index.html">
                    <span class="papertitle">TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation
                      Types</span>
                  </a>
                  <br>
                  Yuhao Lin*,
                  <a href="https://wyl2077.github.io/">Yi-Lin Wei*</a>,
                  Haoran Liao,
                  <strong>Mu Lin</strong>,
                  <a href="https://chengyi-xing.com/">Chengyi Xing</a>,
                  <a href="https://hao-l1.github.io/">Hao Li</a>,
                  <a href="https://www.intelligentrobotics-acrossscales.com/">Dandan Zhang</a>,
                  <a href="https://profiles.stanford.edu/mark-cutkosky">Mark Cutkosky</a>,
                  <a href="https://www.isee-ai.cn/~zhwshi/">Wei-Shi Zheng†</a>,
                  <br>
                  <em>CoRL</em>, 2025
                  <br>
                  <a href="https://isee-laboratory.github.io/TypeTele/index.html">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2507.01857">arXiv</a>
                  /
                  <a href="https://isee-laboratory.github.io/TypeTele/index.html">code(coming soon)</a>
                  <br>
                  <br>
                  A type-guided dexterous teleoperation system that enables operators to select appropriate manipulation
                  types for handling various objects and tasks.
                </td>
              </tr>


              
            </tbody>
          </table>


          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;">
            <tbody>
              <tr>
                <td>
                  <h2>Honor</h2>
                </td>
              </tr>

            </tbody>
          </table>
          <ul style="line-height: 1.8; margin-top: 12px;">
            <li style="margin-bottom: 16px;"><strong>National Scholarship</strong> (2023, 2024)</li>
            <li style="margin-bottom: 16px;"><strong>ICPC Shenyang Regional</strong> - Bronze Medal (2022)</li>
            <li style="margin-bottom: 16px;"><strong>Top-Tier Undergraduate Program 2.0</strong> - Selected (Top 20/400)
            </li>
          </ul>
</body>

</html>